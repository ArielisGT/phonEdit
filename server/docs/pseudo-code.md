#Pseudo code

- [x] Establecer `yarn scripts` para inicialización del servidor
- [x] Instalar las siguientes dependencias:
  - Express
  - Body parser
  - Nodemon
  - Winston
  - Dotenv
  - Node fetch
- [x] Crear un archivo `index.js`.
- [ ] Crear un servidor usando express en el archivo `index.js`, partiendo de la base del scraper que conseguimos en GitHub: https://github.com/karnadii/gsmarena-scraper-json.
- [ ] Crear un archivo llamado scraper.js, donde usaremos la base del scraper y exportaremos el scraper.
- [ ] Crear una carpeta llamada `lib`
- [ ] Dentro de `lib` crear un logger
- [ ] Crear una carpeta llamada `middleware` que contenga los middlewares para:
  - Logger
  - Que las respuestas salgan en formato **JSON**
- [ ] Decirle al servidor que serialice las peticiones a JSON usando `body-parser`
- [ ] Decirle al servidor que use los middleware anteriormente descritos
- [ ] Setear `host` y `port`.
- [ ] Crear base de datos: 
- [ ] Crear carpeta db, donde contenga dos archivos llamados brands.js y phone.js que contengan los elementos donde almacenar la data.
- [ ] Crear archivos llamados brands-seeds.js y phone-seeds.js, los cuales crearan los seeds respectivos.
- [ ] Crear un directorio llamado **routes** y dentro dos archivos llamados `brands` y `phone`, dentro de los archivos,  crear la ruta específica.
- [ ] Crear un directorio llamado **controllers**, dentro crear dos recursos (`brands` y `phone`) y dentro de cada archivo, crear el controlador.
- [ ] Crear un directorio llamado **models**, dentro crear dos recursos (`brands` y `phone`) y dentro de cada archivo, crear el modelo que manejara las peticiones que haga el scraper.
- [ ] Crear un archivo de configuracion oculto (`.env`) a git que contenga las llaves
- [ ] Decirle a git que no registre el archivo `.env`
- [ ] Devolver la data al cliente.
